{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848fee39-4ccb-4856-93fc-29c3dedbbe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BIDIRECTIONAL TRANSFORMER WITH RELATIVE POSITIONING LAYER + CUSTOM ATTENTION LAYER\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6550e-eb9a-43f1-9b85-8bdbdb9d1c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/10000], Loss: 4.1452 "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "channels = 3\n",
    "embed_dim = 256\n",
    "num_heads = 4\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "N = 30 # number of images of the initial sequence\n",
    "K = 20 # number of images of the output sequences (predicted images)\n",
    "batch_size = 16\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load data\n",
    "data = np.load(\"Modigliani_paintings.npy\")\n",
    "#data = np.load(\"Academic_Art_paintings.npy\")\n",
    "num_samples, image_size, _, channels_check = data.shape\n",
    "assert channels == channels_check, f\"Expected {channels} channels, got {channels_check}\"\n",
    "data = np.transpose(data, (0, 3, 1, 2))\n",
    "image_dim = channels * image_size * image_size\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, N, K):\n",
    "    total_length = N + K\n",
    "    num_sequences = num_samples - total_length + 1\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(num_sequences):\n",
    "        seq = data[i:i + N]\n",
    "        tgt = data[i + N:i + N + K]\n",
    "        sequences.append(seq)\n",
    "        targets.append(tgt)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "train_data, train_targets = create_sequences(data, N, K)\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_targets = torch.tensor(train_targets, dtype=torch.float32)\n",
    "train_data_flat = train_data.view(-1, N, image_dim)\n",
    "train_targets_flat = train_targets.view(-1, K, image_dim)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Custom Multi-Head Attention with Relative Positional Encoding (Fixed)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.max_len = max(N, K)\n",
    "        self.rel_pos_bias = nn.Parameter(torch.randn(self.max_len * 2 - 1, num_heads))\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size, q_len, _ = query.size()\n",
    "        k_len = key.size(1)\n",
    "        \n",
    "        Q = self.q_proj(query).view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(key).view(batch_size, k_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(value).view(batch_size, k_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (batch, heads, q_len, k_len)\n",
    "        \n",
    "        # Fixed: Compute relative positional bias with correct q_len and k_len\n",
    "        positions_q = torch.arange(q_len, device=device)\n",
    "        positions_k = torch.arange(k_len, device=device)\n",
    "        rel_dists = positions_q[:, None] - positions_k[None, :]  # (q_len, k_len)\n",
    "        offset = torch.tensor(self.max_len - 1, device=device)\n",
    "        indices = rel_dists + offset  # Shape: (q_len, k_len)\n",
    "        indices = torch.clamp(indices, 0, 2 * self.max_len - 2)  # Clamp to bounds\n",
    "        rel_pos = self.rel_pos_bias[indices]  # (q_len, k_len, num_heads)\n",
    "        # Ensure rel_pos matches scores shape\n",
    "        scores = scores + rel_pos.permute(2, 0, 1).unsqueeze(0)  # (1, num_heads, q_len, k_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V).transpose(1, 2).contiguous().view(batch_size, q_len, self.embed_dim)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "# Custom Transformer Encoder Layer\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "# Custom Transformer Decoder Layer\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)\n",
    "        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None):\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "# Encoder-Decoder with CNNs\n",
    "class EncoderTransformerDecoder(nn.Module):\n",
    "    def __init__(self, image_size, channels, embed_dim, num_heads, hidden_dim, num_layers, dropout, N, K):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, embed_dim, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            CustomTransformerEncoderLayer(embed_dim, num_heads, hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_cnn_input = nn.Linear(embed_dim, embed_dim * image_size * image_size // 16)\n",
    "        self.decoder_cnn = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embed_dim, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, channels, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            CustomTransformerDecoderLayer(embed_dim, num_heads, hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        batch_size = src.size(0)\n",
    "        src = src.view(batch_size * N, self.channels, self.image_size, self.image_size)\n",
    "        src_emb = self.encoder_cnn(src)\n",
    "        src_emb = src_emb.mean(dim=(2, 3))\n",
    "        src_emb = src_emb.view(batch_size, N, embed_dim)\n",
    "        memory = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            memory = layer(memory)\n",
    "        \n",
    "        batch_size = tgt.size(0)\n",
    "        tgt = tgt.view(batch_size * K, self.channels, self.image_size, self.image_size)\n",
    "        tgt_emb = self.encoder_cnn(tgt)\n",
    "        tgt_emb = tgt_emb.mean(dim=(2, 3))\n",
    "        tgt_emb = tgt_emb.view(batch_size, K, embed_dim)\n",
    "        output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            output = layer(output, memory, tgt_mask=None)\n",
    "        output = self.decoder_cnn_input(output)\n",
    "        output = output.view(batch_size * K, embed_dim, self.image_size // 4, self.image_size // 4)\n",
    "        output = self.decoder_cnn(output)\n",
    "        output = output.view(batch_size, K, self.channels, self.image_size, self.image_size)\n",
    "        return output\n",
    "\n",
    "# Perceptual Loss using VGG\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT).features[:16].eval().to(device)\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = vgg\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred_vgg = self.vgg(pred)\n",
    "        target_vgg = self.vgg(target)\n",
    "        return self.mse(pred_vgg, target_vgg) + self.mse(pred, target)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt)\n",
    "            loss = criterion(output.view(-1, channels, image_size, image_size), \n",
    "                            tgt.view(-1, channels, image_size, image_size))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"\\rEpoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\", end=\" \")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Generate K images\n",
    "def generate_images(model, input_seq, K, image_size, channels):\n",
    "    model.eval()\n",
    "    input_seq = input_seq.to(device)\n",
    "    generated = []\n",
    "    tgt = input_seq[:, -1:, :, :, :]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_size = input_seq.size(0)\n",
    "        src = input_seq.view(batch_size * N, channels, image_size, image_size)\n",
    "        src_emb = model.encoder_cnn(src)\n",
    "        src_emb = src_emb.mean(dim=(2, 3))\n",
    "        src_emb = src_emb.view(batch_size, N, embed_dim)\n",
    "        memory = src_emb\n",
    "        for layer in model.encoder_layers:\n",
    "            memory = layer(memory)\n",
    "        \n",
    "        for _ in range(K):\n",
    "            tgt_mask = None\n",
    "            tgt_flat = tgt.view(-1, channels, image_size, image_size)\n",
    "            tgt_emb = model.encoder_cnn(tgt_flat)\n",
    "            tgt_emb = tgt_emb.mean(dim=(2, 3))\n",
    "            tgt_emb = tgt_emb.view(batch_size, tgt.size(1), embed_dim)\n",
    "            output = tgt_emb\n",
    "            for layer in model.decoder_layers:\n",
    "                output = layer(output, memory, tgt_mask=None)\n",
    "            output = model.decoder_cnn_input(output)\n",
    "            output = output.view(batch_size * tgt.size(1), embed_dim, image_size // 4, image_size // 4)\n",
    "            next_image = model.decoder_cnn(output)\n",
    "            next_image = next_image.view(batch_size, tgt.size(1), channels, image_size, image_size)\n",
    "            generated.append(next_image[:, -1:, :, :, :])\n",
    "            tgt = torch.cat([tgt, generated[-1]], dim=1)\n",
    "    \n",
    "    generated = torch.cat(generated, dim=1)\n",
    "    return generated\n",
    "\n",
    "# Plot images\n",
    "def plot_images(original, generated, N, K, image_size, channels):\n",
    "    original = original.cpu().numpy()\n",
    "    generated = generated.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(35, 10))\n",
    "    for i in range(N):\n",
    "        plt.subplot(1, N + K, i + 1)\n",
    "        img = np.transpose(original[i], (1, 2, 0))\n",
    "        img = np.clip(img, 0, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Original {i+1}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.figure(figsize=(35, 10))\n",
    "    for i in range(K):\n",
    "        plt.subplot(1, N + K, N + i + 1)\n",
    "        img = np.transpose(generated[i], (1, 2, 0))\n",
    "        img = np.clip(img, 0, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Generated {i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    model = EncoderTransformerDecoder(\n",
    "        image_size=image_size,\n",
    "        channels=channels,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        N=N,\n",
    "        K=K\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = PerceptualLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "    \n",
    "    plt.plot(range(1, num_epochs + 1), losses, label=\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    test_idx = np.random.randint(0, len(train_data))\n",
    "    test_input = train_data[test_idx:test_idx+1]\n",
    "    generated_images = generate_images(model, test_input, K, image_size, channels)\n",
    "    plot_images(train_data[test_idx], generated_images, N, K, image_size, channels)\n",
    "    \n",
    "    random_indices = np.random.choice(num_samples, size=N, replace=False)\n",
    "    test_input = torch.tensor(data[random_indices], dtype=torch.float32).unsqueeze(0)\n",
    "    generated_images = generate_images(model, test_input, K, image_size, channels)\n",
    "    plot_images(test_input[0], generated_images, N, K, image_size, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534d216-b97a-4a2b-b365-f7ff59ba0d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f205cf-84be-40ce-ad1d-138b19fb6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "GENERATE A SEQUENCE of K IMAGES FROM A GIVEN INPUT SEQUENCE of N images \n",
    "\"\"\"\n",
    "\n",
    "# Generate K images\n",
    "def generate_images(model, input_seq, K, image_size, channels):\n",
    "    model.eval()\n",
    "    input_seq = input_seq.to(device)\n",
    "    generated = []\n",
    "    tgt = input_seq[:, -1:, :, :, :]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_size = input_seq.size(0)\n",
    "        src = input_seq.view(batch_size * N, channels, image_size, image_size)\n",
    "        src_emb = model.encoder_cnn(src)\n",
    "        src_emb = src_emb.mean(dim=(2, 3))\n",
    "        src_emb = src_emb.view(batch_size, N, embed_dim)\n",
    "        memory = src_emb\n",
    "        for layer in model.encoder_layers:\n",
    "            memory = layer(memory)\n",
    "        \n",
    "        for _ in range(K):\n",
    "            tgt_mask = None\n",
    "            tgt_flat = tgt.view(-1, channels, image_size, image_size)\n",
    "            tgt_emb = model.encoder_cnn(tgt_flat)\n",
    "            tgt_emb = tgt_emb.mean(dim=(2, 3))\n",
    "            tgt_emb = tgt_emb.view(batch_size, tgt.size(1), embed_dim)\n",
    "            output = tgt_emb\n",
    "            for layer in model.decoder_layers:\n",
    "                output = layer(output, memory, tgt_mask=None)\n",
    "            output = model.decoder_cnn_input(output)\n",
    "            output = output.view(batch_size * tgt.size(1), embed_dim, image_size // 4, image_size // 4)\n",
    "            next_image = model.decoder_cnn(output)\n",
    "            next_image = next_image.view(batch_size, tgt.size(1), channels, image_size, image_size)\n",
    "            generated.append(next_image[:, -1:, :, :, :])\n",
    "            tgt = torch.cat([tgt, generated[-1]], dim=1)\n",
    "    \n",
    "    generated = torch.cat(generated, dim=1)\n",
    "    return generated\n",
    "\n",
    "# Plot images\n",
    "def plot_images(original, generated, N, K, image_size, channels):\n",
    "    original = original.cpu().numpy()\n",
    "    generated = generated.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(35, 10))\n",
    "    for i in range(N):\n",
    "        plt.subplot(1, N + K, i + 1)\n",
    "        img = np.transpose(original[i], (1, 2, 0))\n",
    "        img = np.clip(img, 0, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Original {i+1}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.figure(figsize=(35, 10))\n",
    "    for i in range(K):\n",
    "        plt.subplot(1, N + K, N + i + 1)\n",
    "        img = np.transpose(generated[i], (1, 2, 0))\n",
    "        img = np.clip(img, 0, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Generated {i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# main \n",
    "\n",
    "K=6 # GENERATED SEQUENCE of K IMAGES \n",
    "N=6 #FROM A GIVEN INPUT SEQUENCE of N images \n",
    "random_indices = np.random.choice(num_samples, size=N, replace=False)\n",
    "test_input = torch.tensor(data[random_indices], dtype=torch.float32).unsqueeze(0)\n",
    "generated_images = generate_images(model, test_input, K, image_size, channels)\n",
    "plot_images(test_input[0], generated_images, N, K, image_size, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1bd281-3a63-40eb-b153-d599e7336edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d072d-8bfe-41d8-8017-37deb988e4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
